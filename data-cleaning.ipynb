{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remplace 'nom_du_fichier.csv' par le chemin réel de ton fichier CSV\n",
    "chemin_fichier = 'flickr_data2.csv'\n",
    "\n",
    "# Lecture du fichier CSV\n",
    "try:\n",
    "    data = pd.read_csv(chemin_fichier, sep=',')\n",
    "    print(\"Fichier importé avec succès !\")\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Informations générales sur le dataset ###\")\n",
    "print(data.info())\n",
    "\n",
    "# Nombre total de lignes et colonnes\n",
    "print(\"\\nNombre total de lignes et colonnes :\")\n",
    "print(f\"Lignes : {data.shape[0]}, Colonnes : {data.shape[1]}\")\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\n### Statistiques descriptives ###\")\n",
    "#print(data.describe(include='all'))  # Inclut toutes les colonnes\n",
    "\n",
    "# Comptage des valeurs nulles par colonne\n",
    "print(\"\\n### Nombre de valeurs nulles par colonne ###\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Pourcentage de valeurs nulles\n",
    "print(\"\\n### Pourcentage de valeurs nulles par colonne ###\")\n",
    "print((data.isnull().sum() / data.shape[0]) * 100)\n",
    "\n",
    "# Détection des doublons\n",
    "print(\"\\n### Nombre de doublons ###\")\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# Aperçu des premières lignes\n",
    "print(\"\\n### Aperçu des premières lignes ###\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # Suppression des doublons sur toutes les colonnes\n",
    "    data_sans_doublons = data.drop_duplicates()\n",
    "    print(\"Les doublons ont été supprimés du dataset.\")\n",
    "\n",
    "    # Identifier les 3 dernières colonnes\n",
    "    dernieres_colonnes = data_sans_doublons.columns[-3:]  # Noms des 3 dernières colonnes\n",
    "    print(\"### Les 3 dernières colonnes identifiées sont : ###\")\n",
    "    print(dernieres_colonnes)\n",
    "\n",
    "    # Filtrer les lignes où au moins une des 3 dernières colonnes contient une valeur non nulle\n",
    "    lignes_problemes = data_sans_doublons[dernieres_colonnes].notnull().any(axis=1)\n",
    "\n",
    "    lignes_problemes_df = data_sans_doublons[lignes_problemes]\n",
    "\n",
    "    lignes_problemes_df['title_date_concat'] = (\n",
    "    lignes_problemes_df[' title'].astype(str) + '_:' + lignes_problemes_df[' date_upload_minute'].astype(str)\n",
    "    )\n",
    "    # Réassigner les colonnes en utilisant .loc\n",
    "    lignes_problemes_df.loc[:, ' date_upload_minute'] = lignes_problemes_df[' date_upload_hour']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_hour'] = lignes_problemes_df[' date_upload_day']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_day'] = lignes_problemes_df[' date_upload_month']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_month'] = lignes_problemes_df[' date_upload_year']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_year'] = lignes_problemes_df['Unnamed: 16']\n",
    "    lignes_problemes_df.loc[:, ' title'] = lignes_problemes_df['title_date_concat']\n",
    "\n",
    "    lignes_problemes_df['annee'] = lignes_problemes_df[' date_taken_minute']\n",
    "    lignes_problemes_df.loc[:, ' date_taken_minute'] = lignes_problemes_df[' date_taken_hour']\n",
    "    lignes_problemes_df.loc[:,' date_taken_hour'] = lignes_problemes_df[' date_taken_day']\n",
    "    lignes_problemes_df.loc[:,' date_taken_day'] = lignes_problemes_df[' date_taken_month']\n",
    "    lignes_problemes_df.loc[:,' date_taken_month'] = lignes_problemes_df[' date_taken_year']\n",
    "    lignes_problemes_df.loc[:,' date_taken_year'] = lignes_problemes_df['annee']\n",
    "\n",
    "    lignes_problemes_df.drop(columns=['annee', 'title_date_concat', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18' ], inplace=True)\n",
    "\n",
    "    # Liste des IDs à supprimer\n",
    "    ids_a_supprimer = [8744184885, 8715425964]\n",
    "\n",
    "    # Suppression des lignes correspondant aux IDs spécifiés\n",
    "    lignes_problemes_df = lignes_problemes_df[~lignes_problemes_df['id'].isin(ids_a_supprimer)]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lignes_problemes_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_problemes = lignes_problemes_df['id'].tolist()  # Liste des IDs corrigés\n",
    "df_base = data_sans_doublons[~data_sans_doublons['id'].isin(ids_problemes)]\n",
    "df_base.drop(columns=['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18' ], inplace=True)\n",
    "# Ajouter les lignes corrigées au DataFrame de base\n",
    "df_base = pd.concat([df_base, lignes_problemes_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Informations générales sur le dataset ###\")\n",
    "print(df_base.info())\n",
    "\n",
    "# Nombre total de lignes et colonnes\n",
    "print(\"\\nNombre total de lignes et colonnes :\")\n",
    "print(f\"Lignes : {df_base.shape[0]}, Colonnes : {df_base.shape[1]}\")\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\n### Statistiques descriptives ###\")\n",
    "#print(data.describe(include='all'))  # Inclut toutes les colonnes\n",
    "\n",
    "# Comptage des valeurs nulles par colonne\n",
    "print(\"\\n### Nombre de valeurs nulles par colonne ###\")\n",
    "print(df_base.isnull().sum())\n",
    "\n",
    "# Pourcentage de valeurs nulles\n",
    "print(\"\\n### Pourcentage de valeurs nulles par colonne ###\")\n",
    "print((df_base.isnull().sum() / df_base.shape[0]) * 100)\n",
    "\n",
    "# Détection des doublons\n",
    "print(\"\\n### Nombre de doublons ###\")\n",
    "print(df_base.duplicated().sum())\n",
    "\n",
    "# Aperçu des premières lignes\n",
    "print(\"\\n### Aperçu des premières lignes ###\")\n",
    "print(df_base.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Appliquer DBSCAN\n",
    "def perform_dbscan_analysis(data, eps, min_samples):\n",
    "    features = data[[\" lat\", \" long\"]]\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(features)\n",
    "    data[\"cluster\"] = labels\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "\n",
    "    print(f\"Nombre de clusters détectés : {n_clusters}\")\n",
    "    print(f\"Nombre de points isolés (bruit) : {n_noise}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Récursivement subdiviser le plus grand cluster\n",
    "def subdivide_largest_cluster(data, eps, min_samples):\n",
    "    largest_cluster = data[data[\"cluster\"] != -1][\"cluster\"].value_counts().idxmax()\n",
    "    print(f\"Subdivising cluster {largest_cluster}...\")\n",
    "\n",
    "    largest_cluster_points = data[data[\"cluster\"] == largest_cluster]\n",
    "\n",
    "    # Apply DBSCAN to the largest cluster\n",
    "    refined_cluster_data = perform_dbscan_analysis(\n",
    "        largest_cluster_points.copy(), eps=eps, min_samples=min_samples\n",
    "    )\n",
    "\n",
    "    # Update the cluster IDs to avoid overlap\n",
    "    max_cluster_id = data[\"cluster\"].max()\n",
    "    refined_cluster_data[\"cluster\"] = refined_cluster_data[\"cluster\"].apply(\n",
    "        lambda x: x + max_cluster_id + 1 if x != -1 else -1\n",
    "    )\n",
    "\n",
    "    # Merge back refined clusters\n",
    "    data.loc[data[\"cluster\"] == largest_cluster, \"cluster\"] = (\n",
    "        -1\n",
    "    )  # Mark old cluster as noise\n",
    "    data = pd.concat([data, refined_cluster_data])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Visualiser les clusters avec des couleurs correctes\n",
    "def plot_clusters_with_matplotlib(data):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Map unique cluster IDs to consistent colors\n",
    "    unique_clusters = sorted(data[\"cluster\"].unique())\n",
    "    cluster_color_map = {\n",
    "        cluster: plt.get_cmap(\"tab20\")(idx % 20)\n",
    "        for idx, cluster in enumerate(unique_clusters)\n",
    "    }\n",
    "\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_points = data[data[\"cluster\"] == cluster]\n",
    "\n",
    "        # Assign colors to clusters, noise gets black\n",
    "        color = cluster_color_map[cluster] if cluster != -1 else \"black\"\n",
    "\n",
    "        plt.scatter(\n",
    "            cluster_points[\" long\"],\n",
    "            cluster_points[\" lat\"],\n",
    "            s=20,\n",
    "            c=[color],\n",
    "            label=f\"Cluster {cluster}\" if cluster != -1 else \"Noise\",\n",
    "        )\n",
    "\n",
    "    plt.title(\"Visualisation des clusters DBSCAN\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.legend(loc=\"best\", fontsize=\"small\", bbox_to_anchor=(1.05, 1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove noise from the data\n",
    "def remove_noise(data):\n",
    "    filtered_data = data[data[\"cluster\"] != -1]\n",
    "    print(f\"Data after removing noise: {filtered_data.shape[0]} rows.\")\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = df_base.sample(100000)\n",
    "\n",
    "\n",
    "if df_sample is not None:\n",
    "    # Perform initial DBSCAN\n",
    "    clustered_data = perform_dbscan_analysis(df_sample, eps=0.0061, min_samples=4)\n",
    "\n",
    "    # Plot initial clusters\n",
    "    plot_clusters_with_matplotlib(clustered_data)\n",
    "\n",
    "    # Subdivide the largest clusters\n",
    "    clustered_data = subdivide_largest_cluster(\n",
    "        clustered_data, eps=0.001, min_samples=4\n",
    "    )\n",
    "    clustered_data = subdivide_largest_cluster(\n",
    "        clustered_data, eps=0.0006, min_samples=4\n",
    "    )\n",
    "\n",
    "    # Plot clusters after subdivision\n",
    "    plot_clusters_with_matplotlib(clustered_data)\n",
    "\n",
    "    # # Save clustered data\n",
    "    # clustered_data.to_csv(output_file_path_clusters, index=False)\n",
    "    # print(f\"Clustered data saved to {output_file_path_clusters}\")\n",
    "\n",
    "    clustered_data_no_noise = remove_noise(clustered_data)\n",
    "    \n",
    "    # # Remove noise and save\n",
    "    # clustered_data_no_noise.to_csv(output_file_path_no_noise, index=False)\n",
    "    # print(f\"Clustered data without noise saved to {output_file_path_no_noise}\")\n",
    "\n",
    "    # Plot clusters without noise\n",
    "    plot_clusters_with_matplotlib(clustered_data_no_noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques sur les clusters\n",
    "print(\"\\n### Statistiques des clusters : ###\")\n",
    "clusters_statistiques = df_sample['cluster'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Affichage formaté des résultats\n",
    "for cluster_id, count in clusters_statistiques.items():\n",
    "    print(f\"Cluster {cluster_id} a {count} lignes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "# Liste des mots à exclure (stopwords et termes spécifiques)\n",
    "stopwords = set(['de', 'chaos', '[lyon', 'france]', ',', 'biennale', 'paper', 'abode', 'pasted', '2011', 'of', 'the', 'by', 'thierry', 'ehrmann', 'france', 'la', 'du', 'et', 'des', 'le', 'les', 'à', 'en', 'un', 'une', 'pour', 'avec', 'dans', 'sur', 'par', 'au', 'aux', 'ce', 'cet', 'cette', 'son', 'sa', 'se', 'ou', 'lyon', 'france'])\n",
    "\n",
    "# Nettoyage supplémentaire pour retirer les apostrophes et autres symboles\n",
    "\n",
    "def tokenize_column(column):\n",
    "    text = column.str.cat(sep=', ').replace(',', ' ')\n",
    "    text = re.sub(r\"['’\\+%-/:]\", \"\", text)  # Retirer les apostrophes et guillemets\n",
    "    words = text.split()\n",
    "    return [word.lower() for word in words if word.lower() not in stopwords]\n",
    "\n",
    "tags_words = tokenize_column(df_sample[' tags'])\n",
    "title_words = tokenize_column(df_sample[' title'])\n",
    "\n",
    "# Compter la fréquence des mots après suppression des mots à exclure\n",
    "tags_counter = Counter(tags_words)\n",
    "title_counter = Counter(title_words)\n",
    "\n",
    "# Affichage des mots les plus fréquents\n",
    "def plot_word_frequencies(counter, column_name):\n",
    "    most_common = counter.most_common(10)\n",
    "    words, counts = zip(*most_common)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, counts)\n",
    "    plt.title(f\"Top 10 mots les plus fréquents dans {column_name}\")\n",
    "    plt.xlabel(\"Mots\")\n",
    "    plt.ylabel(\"Fréquence\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_frequencies(tags_counter, 'tags')\n",
    "plot_word_frequencies(title_counter, 'title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage supplémentaire pour retirer les apostrophes et autres symboles\n",
    "def tokenize_column(column):\n",
    "    text = column.str.cat(sep=', ').replace(',', ' ')\n",
    "    text = re.sub(r\"['’\\-/:]\", \"\", text)  # Retirer les apostrophes et guillemets\n",
    "    words = text.split()\n",
    "    return [word.lower() for word in words if word.lower() not in stopwords]\n",
    "\n",
    "# Liste des mots à exclure (stopwords et termes spécifiques)\n",
    "stopwords = set(['de', 'chaos', '[lyon', 'france]', ',', 'biennale', 'paper', 'abode', 'pasted', '2011', 'of', 'the', 'by', 'france', 'la', 'du', 'et', 'des', 'le', 'les', 'à', 'en', 'un', 'une', 'pour', 'avec', 'dans', 'sur', 'par', 'au', 'aux', 'ce', 'cet', 'cette', 'son', 'sa', 'se', 'ou', 'lyon', 'france'])\n",
    "\n",
    "# Nettoyage supplémentaire pour retirer les apostrophes, les nombres et les mots d'une seule lettre\n",
    "def tokenize_column(column):\n",
    "    text = column.str.cat(sep=', ').replace(',', ' ')\n",
    "    text = re.sub(r\"['’\\-+/:]\", \"\", text)  # Retirer les apostrophes et guillemets\n",
    "    words = text.split()\n",
    "    return [word.lower() for word in words if word.lower() not in stopwords and len(word) > 1 and not any(char.isdigit() for char in word)]\n",
    "\n",
    "# Identifier les mots les plus fréquents dans chaque cluster\n",
    "def get_top_words_by_cluster(df, cluster_col, text_col, top_n=10):\n",
    "    clusters = df[cluster_col].unique()\n",
    "    cluster_top_words = {}\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cluster_data = df[df[cluster_col] == cluster][text_col]\n",
    "        words = tokenize_column(cluster_data)\n",
    "        word_counter = Counter(words)\n",
    "        cluster_top_words[cluster] = word_counter.most_common(top_n)\n",
    "\n",
    "    return cluster_top_words\n",
    "\n",
    "# Calculer la moyenne de fréquence des mots les plus communs sur tous les clusters\n",
    "def get_average_top_words(cluster_top_words):\n",
    "    word_frequencies = Counter()\n",
    "    cluster_count = len(cluster_top_words)\n",
    "\n",
    "    for words in cluster_top_words.values():\n",
    "        for word, freq in words:\n",
    "            word_frequencies[word] += freq\n",
    "\n",
    "    # Moyenne des fréquences\n",
    "    average_frequencies = {word: freq / cluster_count for word, freq in word_frequencies.items()}\n",
    "    return average_frequencies\n",
    "\n",
    "# Fonction pour afficher les mots les plus fréquents par cluster\n",
    "def plot_word_frequencies(counter, cluster_name):\n",
    "    most_common = counter.most_common(10)\n",
    "    if most_common:\n",
    "        words, counts = zip(*most_common)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(words, counts)\n",
    "        plt.title(f\"Top 10 mots les plus fréquents dans le cluster {cluster_name}\")\n",
    "        plt.xlabel(\"Mots\")\n",
    "        plt.ylabel(\"Fréquence\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "def plot_average_word_frequencies(average_top_words):\n",
    "    sorted_avg = sorted(average_top_words.items(), key=lambda x: x[1], reverse=True)\n",
    "    words, counts = zip(*sorted_avg[:10])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(words, counts)\n",
    "    plt.title(\"Moyenne des fréquences des mots les plus communs sur tous les clusters\")\n",
    "    plt.xlabel(\"Mots\")\n",
    "    plt.ylabel(\"Fréquence moyenne\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple d'utilisation (df_sample est le DataFrame contenant les données)\n",
    "cluster_top_words = get_top_words_by_cluster(df_sample, 'cluster', ' title')\n",
    "average_top_words = get_average_top_words(cluster_top_words)\n",
    "\n",
    "# Affichage des résultats et plots\n",
    "print(\"\\nTop mots par cluster:\")\n",
    "for cluster, words in cluster_top_words.items():\n",
    "    print(f\"Cluster {cluster}: {words}\")\n",
    "    word_counter = Counter(dict(words))\n",
    "    plot_word_frequencies(word_counter, cluster)\n",
    "\n",
    "plot_average_word_frequencies(average_top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de spaCy\n",
    "import spacy\n",
    "\n",
    "# Charger le modèle français de spaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Ajuster la limite de longueur pour éviter l'erreur liée à de longs textes\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "# Fonction pour tokeniser avec spaCy en traitant le texte par morceaux\n",
    "\n",
    "def spacy_tokenize_column(column, chunk_size=500000):\n",
    "    text = column.str.cat(sep=' ')\n",
    "    tokens = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        doc = nlp(text[i:i+chunk_size])\n",
    "        tokens.extend([token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.like_num])\n",
    "    return tokens\n",
    "\n",
    "# Identifier les mots les plus fréquents dans chaque cluster avec spaCy\n",
    "def get_top_words_by_cluster_spacy(df, cluster_col, text_col, top_n=10):\n",
    "    clusters = df[cluster_col].unique()\n",
    "    cluster_top_words = {}\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cluster_data = df[df[cluster_col] == cluster][text_col]\n",
    "        tokens = spacy_tokenize_column(cluster_data)\n",
    "        word_counter = Counter(tokens)\n",
    "        cluster_top_words[cluster] = word_counter.most_common(top_n)\n",
    "\n",
    "    return cluster_top_words\n",
    "\n",
    "# Calculer la moyenne de fréquence des mots les plus communs sur tous les clusters\n",
    "def get_average_top_words_spacy(cluster_top_words):\n",
    "    word_frequencies = Counter()\n",
    "    cluster_count = len(cluster_top_words)\n",
    "\n",
    "    for words in cluster_top_words.values():\n",
    "        for word, freq in words:\n",
    "            word_frequencies[word] += freq\n",
    "\n",
    "    # Moyenne des fréquences\n",
    "    average_frequencies = {word: freq / cluster_count for word, freq in word_frequencies.items()}\n",
    "    return average_frequencies\n",
    "\n",
    "# Exemple d'utilisation (df_sample est le DataFrame contenant les données)\n",
    "cluster_top_words_spacy = get_top_words_by_cluster_spacy(df_sample, 'cluster', ' title')\n",
    "average_top_words_spacy = get_average_top_words_spacy(cluster_top_words_spacy)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nTop mots par cluster (spaCy):\")\n",
    "for cluster, words in cluster_top_words_spacy.items():\n",
    "    print(f\"Cluster {cluster}: {words}\")\n",
    "\n",
    "print(\"\\nMoyenne des fréquences des mots les plus communs sur tous les clusters (spaCy):\")\n",
    "for word, avg_freq in sorted(average_top_words_spacy.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word}: {avg_freq:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# Fonction pour afficher une heatmap des mots par cluster\n",
    "def plot_word_heatmap(cluster_top_words):\n",
    "    data = []\n",
    "    for cluster, words in cluster_top_words.items():\n",
    "        for word, freq in words:\n",
    "            data.append([cluster, word, freq])\n",
    "    \n",
    "    df_heatmap = pd.DataFrame(data, columns=['Cluster', 'Mot', 'Fréquence'])\n",
    "    df_pivot = df_heatmap.pivot(index='Mot', columns='Cluster', values='Fréquence').fillna(0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(df_pivot, cmap=\"Blues\", annot=True, fmt=\".1f\")\n",
    "    plt.title(\"Heatmap des mots les plus fréquents par cluster\")\n",
    "    plt.xlabel(\"Cluster\")\n",
    "    plt.ylabel(\"Mot\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation (df_sample est le DataFrame contenant les données)\n",
    "cluster_top_words_spacy = get_top_words_by_cluster_spacy(df_sample, 'cluster', ' title')\n",
    "average_top_words_spacy = get_average_top_words_spacy(cluster_top_words_spacy)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nTop mots par cluster (spaCy):\")\n",
    "for cluster, words in cluster_top_words_spacy.items():\n",
    "    print(f\"Cluster {cluster}: {words}\")\n",
    "\n",
    "print(\"\\nMoyenne des fréquences des mots les plus communs sur tous les clusters (spaCy):\")\n",
    "for word, avg_freq in sorted(average_top_words_spacy.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word}: {avg_freq:.2f}\")\n",
    "\n",
    "# Affichage de la heatmap des mots par cluster\n",
    "plot_word_heatmap(cluster_top_words_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de spaCy et des bibliothèques de visualisation\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Charger le modèle français de spaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Ajuster la limite de longueur pour éviter l'erreur liée à de longs textes\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "# Fonction pour tokeniser avec spaCy en traitant le texte par morceaux\n",
    "def spacy_tokenize_column(column, chunk_size=500000):\n",
    "    text = column.str.cat(sep=' ')\n",
    "    tokens = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        doc = nlp(text[i:i+chunk_size])\n",
    "        tokens.extend([token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.like_num])\n",
    "    return tokens\n",
    "\n",
    "# Identifier les mots les plus fréquents dans chaque cluster avec spaCy\n",
    "def get_top_words_by_cluster_spacy(df, cluster_col, text_col, top_n=10):\n",
    "    clusters = df[cluster_col].unique()\n",
    "    cluster_top_words = {}\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cluster_data = df[df[cluster_col] == cluster][text_col]\n",
    "        tokens = spacy_tokenize_column(cluster_data)\n",
    "        word_counter = Counter(tokens)\n",
    "        cluster_top_words[cluster] = word_counter.most_common(top_n)\n",
    "\n",
    "    return cluster_top_words\n",
    "\n",
    "# Calculer la moyenne de fréquence des mots les plus communs sur tous les clusters\n",
    "def get_average_top_words_spacy(cluster_top_words):\n",
    "    word_frequencies = Counter()\n",
    "    cluster_count = len(cluster_top_words)\n",
    "\n",
    "    for words in cluster_top_words.values():\n",
    "        for word, freq in words:\n",
    "            word_frequencies[word] += freq\n",
    "\n",
    "    # Moyenne des fréquences\n",
    "    average_frequencies = {word: freq / cluster_count for word, freq in word_frequencies.items()}\n",
    "    return average_frequencies\n",
    "\n",
    "# Fonction pour afficher un word cloud des mots les plus fréquents\n",
    "def plot_word_cloud(average_top_words):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(average_top_words)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud des mots les plus fréquents\")\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation (df_sample est le DataFrame contenant les données)\n",
    "cluster_top_words_spacy = get_top_words_by_cluster_spacy(df_sample, 'cluster', ' title')\n",
    "average_top_words_spacy = get_average_top_words_spacy(cluster_top_words_spacy)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nTop mots par cluster (spaCy):\")\n",
    "for cluster, words in cluster_top_words_spacy.items():\n",
    "    print(f\"Cluster {cluster}: {words}\")\n",
    "\n",
    "print(\"\\nMoyenne des fréquences des mots les plus communs sur tous les clusters (spaCy):\")\n",
    "for word, avg_freq in sorted(average_top_words_spacy.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{word}: {avg_freq:.2f}\")\n",
    "\n",
    "# Affichage du word cloud\n",
    "plot_word_cloud(average_top_words_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de spaCy et des bibliothèques de visualisation\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "# Charger le modèle français de spaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Ajuster la limite de longueur pour éviter l'erreur liée à de longs textes\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "# Fonction pour extraire des groupes de mots (n-grams)\n",
    "def extract_ngrams(text, n=2):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Fonction pour tokeniser et extraire des groupes de mots\n",
    "def spacy_tokenize_column(column, chunk_size=500000, ngram_size=2):\n",
    "    text = column.str.cat(sep=' ')\n",
    "    ngrams = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk_text = text[i:i+chunk_size]\n",
    "        ngrams.extend(extract_ngrams(chunk_text, ngram_size))\n",
    "    return ngrams\n",
    "\n",
    "# Identifier les groupes de mots les plus fréquents dans chaque cluster\n",
    "def get_top_ngrams_by_cluster(df, cluster_col, text_col, ngram_size=2, top_n=10):\n",
    "    clusters = df[cluster_col].unique()\n",
    "    cluster_top_ngrams = {}\n",
    "\n",
    "    for cluster in clusters:\n",
    "        cluster_data = df[df[cluster_col] == cluster][text_col]\n",
    "        ngrams = spacy_tokenize_column(cluster_data, ngram_size=ngram_size)\n",
    "        ngram_counter = Counter(ngrams)\n",
    "        cluster_top_ngrams[cluster] = ngram_counter.most_common(top_n)\n",
    "\n",
    "    return cluster_top_ngrams\n",
    "\n",
    "# Calculer la moyenne de fréquence des groupes de mots les plus communs\n",
    "def get_average_top_ngrams(cluster_top_ngrams):\n",
    "    ngram_frequencies = Counter()\n",
    "    cluster_count = len(cluster_top_ngrams)\n",
    "\n",
    "    for ngrams in cluster_top_ngrams.values():\n",
    "        for ngram, freq in ngrams:\n",
    "            ngram_frequencies[ngram] += freq\n",
    "\n",
    "    # Moyenne des fréquences\n",
    "    average_frequencies = {ngram: freq / cluster_count for ngram, freq in ngram_frequencies.items()}\n",
    "    return average_frequencies\n",
    "\n",
    "# Fonction pour afficher un word cloud des groupes de mots les plus fréquents\n",
    "def plot_word_cloud(average_top_ngrams):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(average_top_ngrams)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Word Cloud des groupes de mots les plus fréquents\")\n",
    "    plt.show()\n",
    "\n",
    "# Exemple d'utilisation (df_sample est le DataFrame contenant les données)\n",
    "cluster_top_ngrams_spacy = get_top_ngrams_by_cluster(df_sample, 'cluster', ' title', ngram_size=2)\n",
    "average_top_ngrams_spacy = get_average_top_ngrams(cluster_top_ngrams_spacy)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nTop groupes de mots par cluster (spaCy):\")\n",
    "for cluster, ngrams in cluster_top_ngrams_spacy.items():\n",
    "    print(f\"Cluster {cluster}: {ngrams}\")\n",
    "\n",
    "print(\"\\nMoyenne des fréquences des groupes de mots les plus communs sur tous les clusters (spaCy):\")\n",
    "for ngram, avg_freq in sorted(average_top_ngrams_spacy.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{ngram}: {avg_freq:.2f}\")\n",
    "\n",
    "# Affichage du word cloud\n",
    "plot_word_cloud(average_top_ngrams_spacy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_problemes = lignes_problemes_df['id'].tolist()  # Liste des IDs corrigés\n",
    "df_base = data_sans_doublons[~data_sans_doublons['id'].isin(ids_problemes)]\n",
    "df_base.drop(columns=['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18' ], inplace=True)\n",
    "# Ajouter les lignes corrigées au DataFrame de base\n",
    "df_base = pd.concat([df_base, lignes_problemes_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de spaCy et des bibliothèques de visualisation\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "# Charger le modèle français de spaCy\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "\n",
    "# Ajuster la limite de longueur pour éviter l'erreur liée à de longs textes\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "# Fonction pour filtrer les n-grams contenant \"Lyon\" ou \"France\"\n",
    "def filter_ngrams(ngrams, banned_words={\"lyon\", \"france\", \"paper\"}, threshold=0.5):\n",
    "    filtered_ngrams = []\n",
    "    \n",
    "    for ngram in ngrams:\n",
    "        words = ngram.split()\n",
    "        banned_count = sum(1 for word in words if word.lower() in banned_words)\n",
    "        \n",
    "        if banned_count == 0 or (banned_count / len(words)) < threshold:\n",
    "            filtered_ngrams.append(ngram)\n",
    "    \n",
    "    return filtered_ngrams\n",
    "\n",
    "# Fonction pour extraire des groupes de mots (n-grams) en appliquant le filtre\n",
    "def extract_ngrams(text, n=2):\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    extracted_ngrams = [\" \".join(ngram) for ngram in ngrams]\n",
    "    \n",
    "    return filter_ngrams(extracted_ngrams)\n",
    "\n",
    "# Fonction pour tokeniser et extraire des groupes de mots\n",
    "def spacy_tokenize_column(column, chunk_size=500000, ngram_size=2):\n",
    "    text = column.str.cat(sep=' ')\n",
    "    ngrams = []\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        chunk_text = text[i:i+chunk_size]\n",
    "        ngrams.extend(extract_ngrams(chunk_text, ngram_size))\n",
    "    return ngrams\n",
    "\n",
    "# Identifier les groupes de mots les plus fréquents par cluster et par mois\n",
    "def get_top_ngrams_by_cluster_month(df, cluster_col, text_col, ngram_size=2, time_col='date_taken', top_n=10):\n",
    "    df = df[(df['date_taken'].dt.year >= 2003) & (df['date_taken'].dt.year <= 2019)]  # Filtrer entre 2003 et 2019\n",
    "    df['month'] = df[time_col].dt.month  # Extraire le mois uniquement\n",
    "    clusters = df[cluster_col].unique()\n",
    "    cluster_month_ngrams = {}\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        cluster_data = df[df[cluster_col] == cluster]\n",
    "        month_ngrams = {}\n",
    "        \n",
    "        for month, month_data in cluster_data.groupby('month'):\n",
    "            tokens = spacy_tokenize_column(month_data[text_col], ngram_size=ngram_size)\n",
    "            ngram_counter = Counter(tokens)\n",
    "            month_ngrams[month] = ngram_counter.most_common(top_n)\n",
    "        \n",
    "        cluster_month_ngrams[cluster] = month_ngrams\n",
    "    \n",
    "    return cluster_month_ngrams\n",
    "\n",
    "# Fonction pour afficher une évolution temporelle des groupes de mots par mois\n",
    "def plot_ngram_trends_by_month(cluster_month_ngrams, cluster=None, top_n=5):\n",
    "    if cluster is None or cluster not in cluster_month_ngrams:\n",
    "        cluster = next(iter(cluster_month_ngrams))  # Sélectionner le premier cluster valide\n",
    "        print(f\"Cluster par défaut sélectionné : {cluster}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Obtenir les groupes de mots les plus fréquents sur toutes les périodes\n",
    "    all_ngrams = Counter()\n",
    "    for month, ngrams in cluster_month_ngrams[cluster].items():\n",
    "        for ngram, freq in ngrams:\n",
    "            all_ngrams[ngram] += freq\n",
    "    \n",
    "    most_common_ngrams = [ngram for ngram, _ in all_ngrams.most_common(top_n)]\n",
    "    \n",
    "    # Construire une matrice temporelle\n",
    "    time_series = {ngram: [] for ngram in most_common_ngrams}\n",
    "    months = sorted(cluster_month_ngrams[cluster].keys())\n",
    "    \n",
    "    for month in months:\n",
    "        ngram_counts = dict(cluster_month_ngrams[cluster][month])\n",
    "        for ngram in most_common_ngrams:\n",
    "            time_series[ngram].append(ngram_counts.get(ngram, 0))\n",
    "    \n",
    "    # Tracer les courbes d'évolution des groupes de mots les plus fréquents\n",
    "    for ngram, counts in time_series.items():\n",
    "        plt.plot(months, counts, marker='o', label=ngram)\n",
    "        plt.xlabel(\"Mois\")\n",
    "        plt.ylabel(\"Fréquence\")\n",
    "        plt.title(f\"Évolution des groupes de mots les plus fréquents par mois - Cluster {cluster}\")\n",
    "        plt.legend()\n",
    "        plt.xticks(ticks=range(1, 13), labels=['Jan', 'Fév', 'Mar', 'Avr', 'Mai', 'Juin', 'Juil', 'Août', 'Sep', 'Oct', 'Nov', 'Déc'])\n",
    "        plt.show()\n",
    "\n",
    "# Exemple d'utilisation (df_sample est le DataFrame contenant les données)\n",
    "cluster_month_ngrams_spacy = get_top_ngrams_by_cluster_month(df_base2, 'cluster', 'title', ngram_size=2, time_col='date_taken')\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\nTop groupes de mots par cluster et par mois (spaCy):\")\n",
    "print(\"Clusters disponibles :\", cluster_month_ngrams_spacy.keys())\n",
    "for cluster, month_data in cluster_month_ngrams_spacy.items():\n",
    "    print(f\"Cluster {cluster}:\")\n",
    "    for month, ngrams in month_data.items():\n",
    "        print(f\"  Mois {month}: {ngrams}\")\n",
    "\n",
    "# Visualiser l'évolution des n-grams pour un cluster existant\n",
    "plot_ngram_trends_by_month(cluster_month_ngrams_spacy, cluster=None, top_n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClusters disponibles :\", cluster_month_ngrams_spacy.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dbscan(data, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Apply DBSCAN with the given parameters and return the clustered data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing latitude and longitude.\n",
    "        eps (float): The epsilon value for DBSCAN.\n",
    "        min_samples (int): The minimum number of samples in a neighborhood.\n",
    "    \"\"\"\n",
    "    coords = data[[\" lat\", \" long\"]].values\n",
    "\n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(coords)\n",
    "\n",
    "    # Add cluster labels to the DataFrame\n",
    "    data[\"cluster\"] = labels\n",
    "\n",
    "    # Count the number of clusters and noise points\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "\n",
    "    print(f\"DBSCAN results: {n_clusters} clusters, {n_noise} noise points\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "eps = 0.0011  # Optimal eps from the k-distance plot\n",
    "min_samples = 4  # Optimal min_samples from experimentation\n",
    "clustered_data = apply_dbscan(df_sample, eps, min_samples)\n",
    "\n",
    "def remove_noise(data):\n",
    "    \"\"\"\n",
    "    Remove noise points (cluster = -1) from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing latitude, longitude, and cluster labels.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset without noise points.\n",
    "    \"\"\"\n",
    "    filtered_data = data[data[\"cluster\"] != -1]\n",
    "    print(f\"Data after removing noise: {filtered_data.shape[0]} rows.\")\n",
    "    return filtered_data\n",
    "\n",
    "def subdivide_largest_cluster(data, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Recursively subdivide the largest cluster using DBSCAN.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing latitude, longitude, and cluster labels.\n",
    "        eps (float): The epsilon value for DBSCAN.\n",
    "        min_samples (int): The minimum number of samples in a neighborhood.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with subdivided clusters.\n",
    "    \"\"\"\n",
    "    # Find the largest cluster (excluding noise)\n",
    "    largest_cluster = data[data[\"cluster\"] != -1][\"cluster\"].value_counts().idxmax()\n",
    "    print(f\"Subdividing cluster {largest_cluster}...\")\n",
    "\n",
    "    # Extract points from the largest cluster\n",
    "    largest_cluster_points = data[data[\"cluster\"] == largest_cluster]\n",
    "\n",
    "    # Apply DBSCAN to the largest cluster\n",
    "    refined_cluster_data = perform_dbscan_analysis(\n",
    "        largest_cluster_points.copy(), eps=eps, min_samples=min_samples\n",
    "    )\n",
    "\n",
    "    # Update the cluster IDs to avoid overlap\n",
    "    max_cluster_id = data[\"cluster\"].max()\n",
    "    refined_cluster_data[\"cluster\"] = refined_cluster_data[\"cluster\"].apply(\n",
    "        lambda x: x + max_cluster_id + 1 if x != -1 else -1\n",
    "    )\n",
    "\n",
    "    # Merge back refined clusters\n",
    "    data.loc[data[\"cluster\"] == largest_cluster, \"cluster\"] = -1  # Mark old cluster as noise\n",
    "    data = pd.concat([data, refined_cluster_data])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0006, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0003, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "# clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "# clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "# clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "# clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "# clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "# clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "# clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "# clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clustered_data_no_noise = remove_noise(clustered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compter le nombre de lignes du df \n",
    "print(clustered_data_no_noise.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = clustered_data_no_noise.copy()\n",
    "df_base2 = df_sample.copy()\n",
    "df_base2.columns = df_base2.columns.str.strip()\n",
    "required_columns = ['date_taken_year', 'date_taken_month', 'date_taken_day']\n",
    "df_base2 = df_base2.dropna(subset=required_columns)\n",
    "print(\"\\n### Nombre de valeurs nulles par colonne ###\")\n",
    "print(df_base2.isnull().sum())\n",
    "\n",
    "# Convertir en entiers uniquement si les valeurs sont valides\n",
    "for col in required_columns:\n",
    "    df_base2[col] = df_base2[col].astype(int, errors='ignore')\n",
    "    \n",
    "\n",
    "# Supprimer les dates invalides\n",
    "df_base2 = df_base2[(df_base2['date_taken_year'] > 2003) & \n",
    "                    (df_base2['date_taken_year'] < 2100) &\n",
    "                    (df_base2['date_taken_month'].between(1, 12)) &\n",
    "                    (df_base2['date_taken_day'].between(1, 31))]\n",
    "    \n",
    "df_temp2 = df_base2[['date_taken_year', 'date_taken_month', 'date_taken_day']].rename(\n",
    "    columns={\n",
    "        'date_taken_year': 'year',\n",
    "        'date_taken_month': 'month',\n",
    "        'date_taken_day': 'day'\n",
    "    }\n",
    ")\n",
    "df_base2['date_taken'] = pd.to_datetime(df_temp2, errors='coerce')\n",
    "df_base2['date_taken'] = pd.to_datetime({\n",
    "    'year': df_base2['date_taken_year'],\n",
    "    'month': df_base2['date_taken_month'],\n",
    "    'day': df_base2['date_taken_day']\n",
    "}, errors='coerce')\n",
    "\n",
    "print(df_base2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base.columns = df_base.columns.str.strip()\n",
    "required_columns = ['date_taken_year', 'date_taken_month', 'date_taken_day']\n",
    "df_base = df_base.dropna(subset=required_columns)\n",
    "print(\"\\n### Nombre de valeurs nulles par colonne ###\")\n",
    "print(df_base.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les types de données après suppression\n",
    "print(\"\\n### Types de données après suppression des NaN ###\")\n",
    "print(df_base[required_columns].dtypes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir en entiers uniquement si les valeurs sont valides\n",
    "for col in required_columns:\n",
    "    df_base[col] = df_base[col].astype(int, errors='ignore')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérifier les valeurs minimales et maximales\n",
    "print(\"\\n### Valeurs min/max dans les colonnes de date ###\")\n",
    "print(df_base[required_columns].agg(['min', 'max']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les dates invalides\n",
    "df_base = df_base[(df_base['date_taken_year'] > 2003) & \n",
    "                    (df_base['date_taken_year'] < 2100) &\n",
    "                    (df_base['date_taken_month'].between(1, 12)) &\n",
    "                    (df_base['date_taken_day'].between(1, 31))]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Vérification finale des types de données ###\")\n",
    "print(df_base[required_columns].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_base[['date_taken_year', 'date_taken_month', 'date_taken_day']].rename(\n",
    "    columns={\n",
    "        'date_taken_year': 'year',\n",
    "        'date_taken_month': 'month',\n",
    "        'date_taken_day': 'day'\n",
    "    }\n",
    ")\n",
    "df_base['date_taken'] = pd.to_datetime(df_temp, errors='coerce')\n",
    "df_base['date_taken'] = pd.to_datetime({\n",
    "    'year': df_base['date_taken_year'],\n",
    "    'month': df_base['date_taken_month'],\n",
    "    'day': df_base['date_taken_day']\n",
    "}, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter une colonne pour le jour de la semaine\n",
    "df_base['day_of_week'] = df_base['date_taken'].dt.day_name()\n",
    "\n",
    "# Créer un calendrier par année\n",
    "years = df_base['date_taken_year'].unique()\n",
    "for year in sorted(years):\n",
    "        df_year = df_base[df_base['date_taken_year'] == year]\n",
    "        heatmap_data = df_year.groupby(['date_taken_month', 'day_of_week']).size().unstack().fillna(0)\n",
    "        \n",
    "        # Réorganiser l'ordre des jours de la semaine\n",
    "        order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        heatmap_data = heatmap_data.reindex(columns=order, fill_value=0)\n",
    "        \n",
    "        # Tracer la heatmap pour chaque année\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(heatmap_data, cmap=\"Blues\", annot=True, fmt=\".0f\", linewidths=0.5)\n",
    "        plt.title(f\"Répartition des photos par jour de la semaine et par mois - Année {year}\")\n",
    "        plt.xlabel(\"Jour de la semaine\")\n",
    "        plt.ylabel(\"Mois\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrer les années à partir de 2003\n",
    "df_base = df_base[df_base['date_taken_year'] >= 2003]\n",
    "    \n",
    "# Créer un calendrier par année\n",
    "years = df_base['date_taken_year'].unique()\n",
    "for year in sorted(years):\n",
    "        df_year = df_base[df_base['date_taken_year'] == year]\n",
    "        heatmap_data = df_year.groupby(['date_taken_month', 'date_taken_day']).size().unstack().fillna(0)\n",
    "        \n",
    "        # Tracer la heatmap pour chaque année\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(heatmap_data, cmap=\"Blues\", annot=True, fmt=\".0f\", linewidths=0.5)\n",
    "        plt.title(f\"Répartition des photos par jour du mois - Année {year}\")\n",
    "        plt.xlabel(\"Jour du mois\")\n",
    "        plt.ylabel(\"Mois\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_base.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import io, base64\n",
    "import folium\n",
    "\n",
    "# Make sure column names are stripped\n",
    "df_base.columns = df_base.columns.str.strip()\n",
    "\n",
    "# Load the French spaCy model (ensure you've installed it with: python -m spacy download fr_core_news_sm)\n",
    "nlp = spacy.load(\"fr_core_news_sm\")\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "# Function to tokenize a text column using spaCy\n",
    "def spacy_tokenize_column(column):\n",
    "    text = \" \".join(column.dropna().astype(str).tolist())\n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_.lower() for token in doc \n",
    "              if not token.is_stop and not token.is_punct and not token.like_num]\n",
    "    return tokens\n",
    "\n",
    "# Function to get the top N words for each cluster (ignores noise with cluster == -1)\n",
    "def get_top_words_by_cluster(df, cluster_col, text_col, top_n=10):\n",
    "    cluster_top_words = {}\n",
    "    for cluster in df[cluster_col].unique():\n",
    "        if cluster == -1:\n",
    "            continue\n",
    "        cluster_text = df[df[cluster_col] == cluster][text_col]\n",
    "        tokens = spacy_tokenize_column(cluster_text)\n",
    "        # Si aucun token n'est trouvé, on effectue une tokenisation plus simple\n",
    "        if not tokens:\n",
    "            tokens = \" \".join(cluster_text.dropna().astype(str).tolist()).split()\n",
    "        counts = Counter(tokens)\n",
    "        cluster_top_words[cluster] = counts.most_common(top_n)\n",
    "    return cluster_top_words\n",
    "\n",
    "\n",
    "# Function to generate word clouds from the top words\n",
    "def generate_wordclouds(cluster_top_words):\n",
    "    wordclouds = {}\n",
    "    for cluster, words in cluster_top_words.items():\n",
    "        freq = dict(words)\n",
    "        if not freq:  # Si aucun mot n'est présent, on utilise un texte par défaut\n",
    "            default_text = \"No Data\"\n",
    "            wc = WordCloud(width=400, height=200, background_color='white')\\\n",
    "                 .generate(default_text)\n",
    "        else:\n",
    "            wc = WordCloud(width=400, height=200, background_color='white')\\\n",
    "                 .generate_from_frequencies(freq)\n",
    "        wordclouds[cluster] = wc\n",
    "    return wordclouds\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to convert a WordCloud image to a base64 string\n",
    "def wordcloud_to_base64(wc):\n",
    "    img = wc.to_image()\n",
    "    buffer = io.BytesIO()\n",
    "    img.save(buffer, format=\"PNG\")\n",
    "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "\n",
    "# Generate top words and word clouds using the 'title' column.\n",
    "# (Make sure your dataframe df_base has a 'cluster' column and a 'title' column.)\n",
    "cluster_top_words = get_top_words_by_cluster(df_base2, 'cluster', 'title', top_n=10)\n",
    "wordclouds = generate_wordclouds(cluster_top_words)\n",
    "\n",
    "# Function to create a Folium map for text visualization.\n",
    "def create_text_folium_map(df, wordclouds):\n",
    "    map_center = [df['lat'].mean(), df['long'].mean()]\n",
    "    fmap_text = folium.Map(location=map_center, zoom_start=12)\n",
    "    \n",
    "    # Loop over each unique cluster (skip noise, which is marked as -1)\n",
    "    for cluster in df['cluster'].unique():\n",
    "        if cluster == -1:\n",
    "            continue\n",
    "        cluster_df = df[df['cluster'] == cluster]\n",
    "        centroid = [cluster_df['lat'].mean(), cluster_df['long'].mean()]\n",
    "        \n",
    "        if cluster in wordclouds:\n",
    "            img_b64 = wordcloud_to_base64(wordclouds[cluster])\n",
    "            popup_html = f\"<h4>Cluster {cluster}</h4><img src='data:image/png;base64,{img_b64}' style='width:300px;'>\"\n",
    "        else:\n",
    "            popup_html = f\"<h4>Cluster {cluster}</h4>No wordcloud available\"\n",
    "        \n",
    "        popup = folium.Popup(popup_html, max_width=300)\n",
    "        folium.CircleMarker(location=centroid, radius=8, popup=popup, color=\"green\", fill=True).add_to(fmap_text)\n",
    "    \n",
    "    return fmap_text\n",
    "\n",
    "# Create the text-based map using df_base and the computed wordclouds.\n",
    "fmap_text = create_text_folium_map(df_base2, wordclouds)\n",
    "fmap_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import TimestampedGeoJson\n",
    "\n",
    "def create_timestamped_geojson(df):\n",
    "    features = []\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row['date_taken']):\n",
    "            continue\n",
    "        feature = {\n",
    "            'type': 'Feature',\n",
    "            'properties': {\n",
    "                'time': row['date_taken'].isoformat(),\n",
    "                'popup': f\"ID: {row['id']}<br>Date: {row['date_taken']}\"\n",
    "            },\n",
    "            'geometry': {\n",
    "                'type': 'Point',\n",
    "                'coordinates': [row['long'], row['lat']]\n",
    "            }\n",
    "        }\n",
    "        features.append(feature)\n",
    "    return {'type': 'FeatureCollection', 'features': features}\n",
    "\n",
    "def add_timestamped_layer(fmap, geojson):\n",
    "    TimestampedGeoJson(\n",
    "        geojson,\n",
    "        period='P1D',  # each time step represents one day\n",
    "        add_last_point=True,\n",
    "        auto_play=False,\n",
    "        loop=False,\n",
    "        max_speed=1,\n",
    "        loop_button=True,\n",
    "        date_options='YYYY-MM-DD',\n",
    "        time_slider_drag_update=True\n",
    "    ).add_to(fmap)\n",
    "    return fmap\n",
    "\n",
    "def create_folium_map(df):\n",
    "    map_center = [df['lat'].mean(), df['long'].mean()]\n",
    "    fmap = folium.Map(location=map_center, zoom_start=12)\n",
    "    \n",
    "    # Create and add the timestamped GeoJSON layer\n",
    "    geojson = create_timestamped_geojson(df)\n",
    "    fmap = add_timestamped_layer(fmap, geojson)\n",
    "    \n",
    "    # Optionally, add a marker for each point\n",
    "    for _, row in df.iterrows():\n",
    "        if pd.isna(row['date_taken']):\n",
    "            continue\n",
    "        folium.CircleMarker(\n",
    "            location=[row['lat'], row['long']],\n",
    "            radius=3,\n",
    "            popup=f\"ID: {row['id']}<br>Date: {row['date_taken']}\",\n",
    "            color='blue',\n",
    "            fill=True\n",
    "        ).add_to(fmap)\n",
    "    \n",
    "    return fmap\n",
    "\n",
    "# Create the map using your cleaned data (ensure df_base contains 'lat', 'long', 'date_taken', and 'id')\n",
    "fmap = create_folium_map(df_base2)\n",
    "fmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Calculer le centre de la carte à partir de la moyenne des latitudes et longitudes de df_base2\n",
    "map_center = [df_base2['lat'].mean(), df_base2['long'].mean()]\n",
    "fmap = folium.Map(location=map_center, zoom_start=12)\n",
    "\n",
    "# Boucle sur chaque cluster et calcul du centroïde pour afficher un marqueur\n",
    "for cluster in df_base2['cluster'].unique():\n",
    "    cluster_df = df_base2[df_base2['cluster'] == cluster]\n",
    "    centroid = [cluster_df['lat'].mean(), cluster_df['long'].mean()]\n",
    "    folium.CircleMarker(\n",
    "        location=centroid,\n",
    "        radius=8,\n",
    "        popup=f\"Cluster {cluster}\",\n",
    "        color=\"blue\",\n",
    "        fill=True,\n",
    "        fill_color=\"blue\"\n",
    "    ).add_to(fmap)\n",
    "\n",
    "fmap\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
