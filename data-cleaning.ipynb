{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remplace 'nom_du_fichier.csv' par le chemin réel de ton fichier CSV\n",
    "chemin_fichier = 'flickr_data2.csv'\n",
    "\n",
    "# Lecture du fichier CSV\n",
    "try:\n",
    "    data = pd.read_csv(chemin_fichier, sep=',')\n",
    "    print(\"Fichier importé avec succès !\")\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Informations générales sur le dataset ###\")\n",
    "print(data.info())\n",
    "\n",
    "# Nombre total de lignes et colonnes\n",
    "print(\"\\nNombre total de lignes et colonnes :\")\n",
    "print(f\"Lignes : {data.shape[0]}, Colonnes : {data.shape[1]}\")\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\n### Statistiques descriptives ###\")\n",
    "#print(data.describe(include='all'))  # Inclut toutes les colonnes\n",
    "\n",
    "# Comptage des valeurs nulles par colonne\n",
    "print(\"\\n### Nombre de valeurs nulles par colonne ###\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Pourcentage de valeurs nulles\n",
    "print(\"\\n### Pourcentage de valeurs nulles par colonne ###\")\n",
    "print((data.isnull().sum() / data.shape[0]) * 100)\n",
    "\n",
    "# Détection des doublons\n",
    "print(\"\\n### Nombre de doublons ###\")\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# Aperçu des premières lignes\n",
    "print(\"\\n### Aperçu des premières lignes ###\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # Suppression des doublons sur toutes les colonnes\n",
    "    data_sans_doublons = data.drop_duplicates()\n",
    "    print(\"Les doublons ont été supprimés du dataset.\")\n",
    "\n",
    "    # Identifier les 3 dernières colonnes\n",
    "    dernieres_colonnes = data_sans_doublons.columns[-3:]  # Noms des 3 dernières colonnes\n",
    "    print(\"### Les 3 dernières colonnes identifiées sont : ###\")\n",
    "    print(dernieres_colonnes)\n",
    "\n",
    "    # Filtrer les lignes où au moins une des 3 dernières colonnes contient une valeur non nulle\n",
    "    lignes_problemes = data_sans_doublons[dernieres_colonnes].notnull().any(axis=1)\n",
    "\n",
    "    lignes_problemes_df = data_sans_doublons[lignes_problemes]\n",
    "\n",
    "    lignes_problemes_df['title_date_concat'] = (\n",
    "    lignes_problemes_df[' title'].astype(str) + '_:' + lignes_problemes_df[' date_upload_minute'].astype(str)\n",
    "    )\n",
    "    # Réassigner les colonnes en utilisant .loc\n",
    "    lignes_problemes_df.loc[:, ' date_upload_minute'] = lignes_problemes_df[' date_upload_hour']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_hour'] = lignes_problemes_df[' date_upload_day']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_day'] = lignes_problemes_df[' date_upload_month']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_month'] = lignes_problemes_df[' date_upload_year']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_year'] = lignes_problemes_df['Unnamed: 16']\n",
    "    lignes_problemes_df.loc[:, ' title'] = lignes_problemes_df['title_date_concat']\n",
    "\n",
    "    lignes_problemes_df['annee'] = lignes_problemes_df[' date_taken_minute']\n",
    "    lignes_problemes_df.loc[:, ' date_taken_minute'] = lignes_problemes_df[' date_taken_hour']\n",
    "    lignes_problemes_df.loc[:,' date_taken_hour'] = lignes_problemes_df[' date_taken_day']\n",
    "    lignes_problemes_df.loc[:,' date_taken_day'] = lignes_problemes_df[' date_taken_month']\n",
    "    lignes_problemes_df.loc[:,' date_taken_month'] = lignes_problemes_df[' date_taken_year']\n",
    "    lignes_problemes_df.loc[:,' date_taken_year'] = lignes_problemes_df['annee']\n",
    "\n",
    "    lignes_problemes_df.drop(columns=['annee', 'title_date_concat', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18' ], inplace=True)\n",
    "\n",
    "    # Liste des IDs à supprimer\n",
    "    ids_a_supprimer = [8744184885, 8715425964]\n",
    "\n",
    "    # Suppression des lignes correspondant aux IDs spécifiés\n",
    "    lignes_problemes_df = lignes_problemes_df[~lignes_problemes_df['id'].isin(ids_a_supprimer)]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lignes_problemes_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_problemes = lignes_problemes_df['id'].tolist()  # Liste des IDs corrigés\n",
    "df_base = data_sans_doublons[~data_sans_doublons['id'].isin(ids_problemes)]\n",
    "df_base.drop(columns=['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18' ], inplace=True)\n",
    "# Ajouter les lignes corrigées au DataFrame de base\n",
    "df_base = pd.concat([df_base, lignes_problemes_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Informations générales sur le dataset ###\")\n",
    "print(df_base.info())\n",
    "\n",
    "# Nombre total de lignes et colonnes\n",
    "print(\"\\nNombre total de lignes et colonnes :\")\n",
    "print(f\"Lignes : {df_base.shape[0]}, Colonnes : {df_base.shape[1]}\")\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\n### Statistiques descriptives ###\")\n",
    "#print(data.describe(include='all'))  # Inclut toutes les colonnes\n",
    "\n",
    "# Comptage des valeurs nulles par colonne\n",
    "print(\"\\n### Nombre de valeurs nulles par colonne ###\")\n",
    "print(df_base.isnull().sum())\n",
    "\n",
    "# Pourcentage de valeurs nulles\n",
    "print(\"\\n### Pourcentage de valeurs nulles par colonne ###\")\n",
    "print((df_base.isnull().sum() / df_base.shape[0]) * 100)\n",
    "\n",
    "# Détection des doublons\n",
    "print(\"\\n### Nombre de doublons ###\")\n",
    "print(df_base.duplicated().sum())\n",
    "\n",
    "# Aperçu des premières lignes\n",
    "print(\"\\n### Aperçu des premières lignes ###\")\n",
    "print(df_base.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Appliquer DBSCAN\n",
    "def perform_dbscan_analysis(data, eps, min_samples):\n",
    "    features = data[[\" lat\", \" long\"]]\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(features)\n",
    "    data[\"cluster\"] = labels\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "\n",
    "    print(f\"Nombre de clusters détectés : {n_clusters}\")\n",
    "    print(f\"Nombre de points isolés (bruit) : {n_noise}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Récursivement subdiviser le plus grand cluster\n",
    "def subdivide_largest_cluster(data, eps, min_samples):\n",
    "    largest_cluster = data[data[\"cluster\"] != -1][\"cluster\"].value_counts().idxmax()\n",
    "    print(f\"Subdivising cluster {largest_cluster}...\")\n",
    "\n",
    "    largest_cluster_points = data[data[\"cluster\"] == largest_cluster]\n",
    "\n",
    "    # Apply DBSCAN to the largest cluster\n",
    "    refined_cluster_data = perform_dbscan_analysis(\n",
    "        largest_cluster_points.copy(), eps=eps, min_samples=min_samples\n",
    "    )\n",
    "\n",
    "    # Update the cluster IDs to avoid overlap\n",
    "    max_cluster_id = data[\"cluster\"].max()\n",
    "    refined_cluster_data[\"cluster\"] = refined_cluster_data[\"cluster\"].apply(\n",
    "        lambda x: x + max_cluster_id + 1 if x != -1 else -1\n",
    "    )\n",
    "\n",
    "    # Merge back refined clusters\n",
    "    data.loc[data[\"cluster\"] == largest_cluster, \"cluster\"] = (\n",
    "        -1\n",
    "    )  # Mark old cluster as noise\n",
    "    data = pd.concat([data, refined_cluster_data])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Visualiser les clusters avec des couleurs correctes\n",
    "def plot_clusters_with_matplotlib(data):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Map unique cluster IDs to consistent colors\n",
    "    unique_clusters = sorted(data[\"cluster\"].unique())\n",
    "    cluster_color_map = {\n",
    "        cluster: plt.get_cmap(\"tab20\")(idx % 20)\n",
    "        for idx, cluster in enumerate(unique_clusters)\n",
    "    }\n",
    "\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_points = data[data[\"cluster\"] == cluster]\n",
    "\n",
    "        # Assign colors to clusters, noise gets black\n",
    "        color = cluster_color_map[cluster] if cluster != -1 else \"black\"\n",
    "\n",
    "        plt.scatter(\n",
    "            cluster_points[\" long\"],\n",
    "            cluster_points[\" lat\"],\n",
    "            s=20,\n",
    "            c=[color],\n",
    "            label=f\"Cluster {cluster}\" if cluster != -1 else \"Noise\",\n",
    "        )\n",
    "\n",
    "    plt.title(\"Visualisation des clusters DBSCAN\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.legend(loc=\"best\", fontsize=\"small\", bbox_to_anchor=(1.05, 1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove noise from the data\n",
    "def remove_noise(data):\n",
    "    filtered_data = data[data[\"cluster\"] != -1]\n",
    "    print(f\"Data after removing noise: {filtered_data.shape[0]} rows.\")\n",
    "    return filtered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sample = df_base.sample(10000)\n",
    "\n",
    "\n",
    "if df_sample is not None:\n",
    "    # Perform initial DBSCAN\n",
    "    clustered_data = perform_dbscan_analysis(df_sample, eps=0.0061, min_samples=4)\n",
    "\n",
    "    # Plot initial clusters\n",
    "    plot_clusters_with_matplotlib(clustered_data)\n",
    "\n",
    "    # Subdivide the largest clusters\n",
    "    clustered_data = subdivide_largest_cluster(\n",
    "        clustered_data, eps=0.001, min_samples=4\n",
    "    )\n",
    "    clustered_data = subdivide_largest_cluster(\n",
    "        clustered_data, eps=0.0006, min_samples=4\n",
    "    )\n",
    "\n",
    "    # Plot clusters after subdivision\n",
    "    plot_clusters_with_matplotlib(clustered_data)\n",
    "\n",
    "    # # Save clustered data\n",
    "    # clustered_data.to_csv(output_file_path_clusters, index=False)\n",
    "    # print(f\"Clustered data saved to {output_file_path_clusters}\")\n",
    "\n",
    "    clustered_data_no_noise = remove_noise(clustered_data)\n",
    "    \n",
    "    # # Remove noise and save\n",
    "    # clustered_data_no_noise.to_csv(output_file_path_no_noise, index=False)\n",
    "    # print(f\"Clustered data without noise saved to {output_file_path_no_noise}\")\n",
    "\n",
    "    # Plot clusters without noise\n",
    "    plot_clusters_with_matplotlib(clustered_data_no_noise)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
