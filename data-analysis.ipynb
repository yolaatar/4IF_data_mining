{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Remplace 'nom_du_fichier.csv' par le chemin réel de ton fichier CSV\n",
    "chemin_fichier = 'flickr_data2.csv'\n",
    "\n",
    "# Lecture du fichier CSV\n",
    "try:\n",
    "    data = pd.read_csv(chemin_fichier, sep=',')\n",
    "    print(\"Fichier importé avec succès !\")\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Informations générales sur le dataset ###\")\n",
    "print(data.info())\n",
    "\n",
    "# Nombre total de lignes et colonnes\n",
    "print(\"\\nNombre total de lignes et colonnes :\")\n",
    "print(f\"Lignes : {data.shape[0]}, Colonnes : {data.shape[1]}\")\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"\\n### Statistiques descriptives ###\")\n",
    "#print(data.describe(include='all'))  # Inclut toutes les colonnes\n",
    "\n",
    "# Comptage des valeurs nulles par colonne\n",
    "print(\"\\n### Nombre de valeurs nulles par colonne ###\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Pourcentage de valeurs nulles\n",
    "print(\"\\n### Pourcentage de valeurs nulles par colonne ###\")\n",
    "print((data.isnull().sum() / data.shape[0]) * 100)\n",
    "\n",
    "# Détection des doublons\n",
    "print(\"\\n### Nombre de doublons ###\")\n",
    "print(data.duplicated().sum())\n",
    "\n",
    "# Aperçu des premières lignes\n",
    "print(\"\\n### Aperçu des premières lignes ###\")\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # Suppression des doublons sur toutes les colonnes\n",
    "    data_sans_doublons = data.drop_duplicates()\n",
    "    print(\"Les doublons ont été supprimés du dataset.\")\n",
    "\n",
    "    # Identifier les 3 dernières colonnes\n",
    "    dernieres_colonnes = data_sans_doublons.columns[-3:]  # Noms des 3 dernières colonnes\n",
    "    print(\"### Les 3 dernières colonnes identifiées sont : ###\")\n",
    "    print(dernieres_colonnes)\n",
    "\n",
    "    # Filtrer les lignes où au moins une des 3 dernières colonnes contient une valeur non nulle\n",
    "    lignes_problemes = data_sans_doublons[dernieres_colonnes].notnull().any(axis=1)\n",
    "\n",
    "    lignes_problemes_df = data_sans_doublons[lignes_problemes]\n",
    "\n",
    "    lignes_problemes_df['title_date_concat'] = (\n",
    "    lignes_problemes_df[' title'].astype(str) + '_:' + lignes_problemes_df[' date_upload_minute'].astype(str)\n",
    "    )\n",
    "    # Réassigner les colonnes en utilisant .loc\n",
    "    lignes_problemes_df.loc[:, ' date_upload_minute'] = lignes_problemes_df[' date_upload_hour']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_hour'] = lignes_problemes_df[' date_upload_day']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_day'] = lignes_problemes_df[' date_upload_month']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_month'] = lignes_problemes_df[' date_upload_year']\n",
    "    lignes_problemes_df.loc[:, ' date_upload_year'] = lignes_problemes_df['Unnamed: 16']\n",
    "    lignes_problemes_df.loc[:, ' title'] = lignes_problemes_df['title_date_concat']\n",
    "\n",
    "    lignes_problemes_df['annee'] = lignes_problemes_df[' date_taken_minute']\n",
    "    lignes_problemes_df.loc[:, ' date_taken_minute'] = lignes_problemes_df[' date_taken_hour']\n",
    "    lignes_problemes_df.loc[:,' date_taken_hour'] = lignes_problemes_df[' date_taken_day']\n",
    "    lignes_problemes_df.loc[:,' date_taken_day'] = lignes_problemes_df[' date_taken_month']\n",
    "    lignes_problemes_df.loc[:,' date_taken_month'] = lignes_problemes_df[' date_taken_year']\n",
    "    lignes_problemes_df.loc[:,' date_taken_year'] = lignes_problemes_df['annee']\n",
    "\n",
    "    lignes_problemes_df.drop(columns=['annee', 'title_date_concat', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18' ], inplace=True)\n",
    "\n",
    "    # Liste des IDs à supprimer\n",
    "    ids_a_supprimer = [8744184885, 8715425964]\n",
    "\n",
    "    # Suppression des lignes correspondant aux IDs spécifiés\n",
    "    lignes_problemes_df = lignes_problemes_df[~lignes_problemes_df['id'].isin(ids_a_supprimer)]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Une erreur est survenue : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_problemes = lignes_problemes_df['id'].tolist()  # Liste des IDs corrigés\n",
    "df_base = data_sans_doublons[~data_sans_doublons['id'].isin(ids_problemes)]\n",
    "df_base.drop(columns=['Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18' ], inplace=True)\n",
    "# Ajouter les lignes corrigées au DataFrame de base\n",
    "df_base = pd.concat([df_base, lignes_problemes_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_base.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_troncate = df_base.iloc[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Garder seulement les 4 premières colonnes\n",
    "df_troncate = df_base.iloc[:, :4]\n",
    "\n",
    "# Vérification des colonnes conservées\n",
    "print(\"### Aperçu du DataFrame réduit aux 4 premières colonnes : ###\")\n",
    "print(df_troncate.head())\n",
    "\n",
    "# Préparation des données pour la clusterisation (Latitude et Longitude)\n",
    "coords = df_troncate[[' lat', ' long']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "# Charger les données nettoyées\n",
    "def load_cleaned_data(file_path):\n",
    "    try:\n",
    "        data = pd.read_csv(file_path, nrows=10000)\n",
    "        print(f\"Loaded data with {data.shape[0]} rows and {data.shape[1]} columns.\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading cleaned data: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Appliquer DBSCAN\n",
    "def perform_dbscan_analysis(data, eps, min_samples):\n",
    "    features = data[[\" lat\", \" long\"]]\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(features)\n",
    "    data[\"cluster\"] = labels\n",
    "\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "\n",
    "    print(f\"Nombre de clusters détectés : {n_clusters}\")\n",
    "    print(f\"Nombre de points isolés (bruit) : {n_noise}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Récursivement subdiviser le plus grand cluster\n",
    "def subdivide_largest_cluster(data, eps, min_samples):\n",
    "    largest_cluster = data[data[\"cluster\"] != -1][\"cluster\"].value_counts().idxmax()\n",
    "    print(f\"Subdivising cluster {largest_cluster}...\")\n",
    "\n",
    "    largest_cluster_points = data[data[\"cluster\"] == largest_cluster]\n",
    "\n",
    "    # Apply DBSCAN to the largest cluster\n",
    "    refined_cluster_data = perform_dbscan_analysis(\n",
    "        largest_cluster_points.copy(), eps=eps, min_samples=min_samples\n",
    "    )\n",
    "\n",
    "    # Update the cluster IDs to avoid overlap\n",
    "    max_cluster_id = data[\"cluster\"].max()\n",
    "    refined_cluster_data[\"cluster\"] = refined_cluster_data[\"cluster\"].apply(\n",
    "        lambda x: x + max_cluster_id + 1 if x != -1 else -1\n",
    "    )\n",
    "\n",
    "    # Merge back refined clusters\n",
    "    data.loc[data[\"cluster\"] == largest_cluster, \"cluster\"] = (\n",
    "        -1\n",
    "    )  # Mark old cluster as noise\n",
    "    data = pd.concat([data, refined_cluster_data])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Visualiser les clusters avec des couleurs correctes\n",
    "def plot_clusters_with_matplotlib(data):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Map unique cluster IDs to consistent colors\n",
    "    unique_clusters = sorted(data[\"cluster\"].unique())\n",
    "    cluster_color_map = {\n",
    "        cluster: plt.get_cmap(\"tab20\")(idx % 20)\n",
    "        for idx, cluster in enumerate(unique_clusters)\n",
    "    }\n",
    "\n",
    "    for cluster in unique_clusters:\n",
    "        cluster_points = data[data[\"cluster\"] == cluster]\n",
    "\n",
    "        # Assign colors to clusters, noise gets black\n",
    "        color = cluster_color_map[cluster] if cluster != -1 else \"black\"\n",
    "\n",
    "        plt.scatter(\n",
    "            cluster_points[\" long\"],\n",
    "            cluster_points[\" lat\"],\n",
    "            s=20,\n",
    "            c=[color],\n",
    "            label=f\"Cluster {cluster}\" if cluster != -1 else \"Noise\",\n",
    "        )\n",
    "\n",
    "    plt.title(\"Visualisation des clusters DBSCAN\")\n",
    "    plt.xlabel(\"Longitude\")\n",
    "    plt.ylabel(\"Latitude\")\n",
    "    plt.legend(loc=\"best\", fontsize=\"small\", bbox_to_anchor=(1.05, 1))\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Remove noise from the data\n",
    "def remove_noise(data):\n",
    "    filtered_data = data[data[\"cluster\"] != -1]\n",
    "    print(f\"Data after removing noise: {filtered_data.shape[0]} rows.\")\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # File paths\n",
    "    input_file_path = \"./data/cleaned_flickr_data.csv\"\n",
    "    output_file_path_clusters = \"./data/clustered_flickr_data.csv\"\n",
    "    output_file_path_no_noise = \"./data/clustered_flickr_data.csv\"\n",
    "\n",
    "    # Load data\n",
    "    df = load_cleaned_data(input_file_path)\n",
    "\n",
    "    if df is not None:\n",
    "        # Perform initial DBSCAN\n",
    "        clustered_data = perform_dbscan_analysis(df, eps=0.0061, min_samples=4)\n",
    "\n",
    "        # Plot initial clusters\n",
    "        plot_clusters_with_matplotlib(clustered_data)\n",
    "\n",
    "        # Subdivide the largest clusters\n",
    "        clustered_data = subdivide_largest_cluster(\n",
    "            clustered_data, eps=0.001, min_samples=4\n",
    "        )\n",
    "        clustered_data = subdivide_largest_cluster(\n",
    "            clustered_data, eps=0.0006, min_samples=4\n",
    "        )\n",
    "\n",
    "        # Plot clusters after subdivision\n",
    "        plot_clusters_with_matplotlib(clustered_data)\n",
    "\n",
    "        # Save clustered data\n",
    "        clustered_data.to_csv(output_file_path_clusters, index=False)\n",
    "        print(f\"Clustered data saved to {output_file_path_clusters}\")\n",
    "\n",
    "        # Remove noise and save\n",
    "        clustered_data_no_noise = remove_noise(clustered_data)\n",
    "        clustered_data_no_noise.to_csv(output_file_path_no_noise, index=False)\n",
    "        print(f\"Clustered data without noise saved to {output_file_path_no_noise}\")\n",
    "\n",
    "        # Plot clusters without noise\n",
    "        plot_clusters_with_matplotlib(clustered_data_no_noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "df_sample = df_troncate.sample(n=100000)\n",
    "\n",
    "def plot_k_distance(data, k=4):\n",
    "    \"\"\"\n",
    "    Plot the k-distance graph to help choose the optimal eps value for DBSCAN.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing latitude and longitude.\n",
    "        k (int): The number of nearest neighbors to consider.\n",
    "    \"\"\"\n",
    "    # Extract latitude and longitude\n",
    "    coords = data[[\" lat\", \" long\"]].values\n",
    "\n",
    "    # Compute the distance to the k-th nearest neighbor\n",
    "    neighbors = NearestNeighbors(n_neighbors=k)\n",
    "    neighbors_fit = neighbors.fit(coords)\n",
    "    distances, _ = neighbors_fit.kneighbors(coords)\n",
    "\n",
    "    # Sort the distances\n",
    "    distances = np.sort(distances[:, k-1], axis=0)\n",
    "\n",
    "    # Plot the k-distance graph\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(distances)\n",
    "    plt.title(f\"K-Distance Graph (k={k})\")\n",
    "    plt.xlabel(\"Points sorted by distance\")\n",
    "    plt.ylabel(f\"{k}-th nearest neighbor distance\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "plot_k_distance(df_sample, k=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def experiment_with_min_samples(data, eps, min_samples_list):\n",
    "    \"\"\"\n",
    "    Experiment with different values of min_samples and visualize the results.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing latitude and longitude.\n",
    "        eps (float): The epsilon value for DBSCAN.\n",
    "        min_samples_list (list): A list of min_samples values to try.\n",
    "    \"\"\"\n",
    "    coords = data[[\" lat\", \" long\"]].values\n",
    "\n",
    "    for min_samples in min_samples_list:\n",
    "        # Apply DBSCAN\n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = dbscan.fit_predict(coords)\n",
    "\n",
    "        # Count the number of clusters and noise points\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        n_noise = list(labels).count(-1)\n",
    "\n",
    "        print(f\"min_samples={min_samples}: {n_clusters} clusters, {n_noise} noise points\")\n",
    "\n",
    "        # Visualize the clusters\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(coords[:, 1], coords[:, 0], c=labels, cmap=\"tab20\", s=20)\n",
    "        plt.title(f\"DBSCAN Clustering (eps={eps}, min_samples={min_samples})\")\n",
    "        plt.xlabel(\"Longitude\")\n",
    "        plt.ylabel(\"Latitude\")\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "eps = 0.0008  # Example value from the k-distance plot\n",
    "min_samples_list = [4, 5, 6]  # Try different values\n",
    "experiment_with_min_samples(df_sample, eps, min_samples_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dbscan(data, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Apply DBSCAN with the given parameters and return the clustered data.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing latitude and longitude.\n",
    "        eps (float): The epsilon value for DBSCAN.\n",
    "        min_samples (int): The minimum number of samples in a neighborhood.\n",
    "    \"\"\"\n",
    "    coords = data[[\" lat\", \" long\"]].values\n",
    "\n",
    "    # Apply DBSCAN\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(coords)\n",
    "\n",
    "    # Add cluster labels to the DataFrame\n",
    "    data[\"cluster\"] = labels\n",
    "\n",
    "    # Count the number of clusters and noise points\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise = list(labels).count(-1)\n",
    "\n",
    "    print(f\"DBSCAN results: {n_clusters} clusters, {n_noise} noise points\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "eps = 0.0011  # Optimal eps from the k-distance plot\n",
    "min_samples = 4  # Optimal min_samples from experimentation\n",
    "clustered_data = apply_dbscan(df_sample, eps, min_samples)\n",
    "\n",
    "def remove_noise(data):\n",
    "    \"\"\"\n",
    "    Remove noise points (cluster = -1) from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing latitude, longitude, and cluster labels.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset without noise points.\n",
    "    \"\"\"\n",
    "    filtered_data = data[data[\"cluster\"] != -1]\n",
    "    print(f\"Data after removing noise: {filtered_data.shape[0]} rows.\")\n",
    "    return filtered_data\n",
    "\n",
    "def subdivide_largest_cluster(data, eps, min_samples):\n",
    "    \"\"\"\n",
    "    Recursively subdivide the largest cluster using DBSCAN.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing latitude, longitude, and cluster labels.\n",
    "        eps (float): The epsilon value for DBSCAN.\n",
    "        min_samples (int): The minimum number of samples in a neighborhood.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: The dataset with subdivided clusters.\n",
    "    \"\"\"\n",
    "    # Find the largest cluster (excluding noise)\n",
    "    largest_cluster = data[data[\"cluster\"] != -1][\"cluster\"].value_counts().idxmax()\n",
    "    print(f\"Subdividing cluster {largest_cluster}...\")\n",
    "\n",
    "    # Extract points from the largest cluster\n",
    "    largest_cluster_points = data[data[\"cluster\"] == largest_cluster]\n",
    "\n",
    "    # Apply DBSCAN to the largest cluster\n",
    "    refined_cluster_data = perform_dbscan_analysis(\n",
    "        largest_cluster_points.copy(), eps=eps, min_samples=min_samples\n",
    "    )\n",
    "\n",
    "    # Update the cluster IDs to avoid overlap\n",
    "    max_cluster_id = data[\"cluster\"].max()\n",
    "    refined_cluster_data[\"cluster\"] = refined_cluster_data[\"cluster\"].apply(\n",
    "        lambda x: x + max_cluster_id + 1 if x != -1 else -1\n",
    "    )\n",
    "\n",
    "    # Merge back refined clusters\n",
    "    data.loc[data[\"cluster\"] == largest_cluster, \"cluster\"] = -1  # Mark old cluster as noise\n",
    "    data = pd.concat([data, refined_cluster_data])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0006, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0003, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "clustered_data = subdivide_largest_cluster(clustered_data, eps=0.0001, min_samples=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clustered_data_no_noise = remove_noise(clustered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "def plot_clusters_on_map(data):\n",
    "    \"\"\"\n",
    "    Plot the clusters on a Folium map.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing latitude, longitude, and cluster labels.\n",
    "    \"\"\"\n",
    "    # Create a map centered on Lyon\n",
    "    lyon_map = folium.Map(location=[45.7640, 4.8357], zoom_start=13)\n",
    "\n",
    "    # Define a color map for clusters\n",
    "    colors = [\n",
    "        \"red\", \"blue\", \"green\", \"purple\", \"orange\", \"darkred\", \"lightblue\",\n",
    "        \"darkgreen\", \"darkblue\", \"pink\", \"lightgreen\", \"gray\", \"black\"\n",
    "    ]\n",
    "\n",
    "    # Add points to the map\n",
    "    for _, row in data.iterrows():\n",
    "        cluster = row[\"cluster\"]\n",
    "        color = colors[cluster % len(colors)] if cluster != -1 else \"black\"\n",
    "        folium.CircleMarker(\n",
    "            location=[row[\" lat\"], row[\" long\"]],\n",
    "            radius=3,\n",
    "            color=color,\n",
    "            fill=True,\n",
    "            fill_color=color,\n",
    "            popup=f\"Cluster {cluster}\"\n",
    "        ).add_to(lyon_map)\n",
    "\n",
    "    return lyon_map\n",
    "\n",
    "# Example usage\n",
    "lyon_map = plot_clusters_on_map(clustered_data_no_noise)\n",
    "lyon_map  # Display the map in a Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_data = data_sans_doublons.sample(1000, random_state=42)\n",
    "#new df with only user lat and long\n",
    "sampled_data = sampled_data[[' lat', ' long']]\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elbow Method\n",
    "inertia = []\n",
    "k_range = range(30, 200)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.xlabel(\"Number of Clusters (k)\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform KMeans clustering with the chosen number of clusters (e.g., 3)\n",
    "kmeans = KMeans(n_clusters=60, random_state=42)\n",
    "kmeans.fit(scaled_data)\n",
    "\n",
    "# Add cluster labels back to the original sampled data\n",
    "sampled_data['Cluster'] = kmeans.labels_\n",
    "\n",
    "# Visualization of clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    sampled_data[' lat'], sampled_data[' long'], \n",
    "    c=sampled_data['Cluster'], cmap='viridis', s=50\n",
    ")\n",
    "plt.scatter(\n",
    "    scaler.inverse_transform(kmeans.cluster_centers_)[:, 0], \n",
    "    scaler.inverse_transform(kmeans.cluster_centers_)[:, 1], \n",
    "    s=300, c='red', label='Centroids'\n",
    ")\n",
    "plt.title(\"KMeans Clustering\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute linkage matrix\n",
    "linkage_matrix = linkage(scaled_data, method='complete')  # 'ward', 'single', 'complete', etc.\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(10, 7))\n",
    "dendrogram(linkage_matrix)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters\n",
    "num_clusters = 170\n",
    "\n",
    "# Assign cluster labels\n",
    "cluster_labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust')\n",
    "\n",
    "# Add cluster labels to the DataFrame\n",
    "sampled_data['Cluster'] = cluster_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(\n",
    "    sampled_data[' lat'], sampled_data[' long'], \n",
    "    c=sampled_data['Cluster'], cmap='viridis', s=50\n",
    ")\n",
    "plt.title(\"Hierarchical Clustering\")\n",
    "plt.xlabel(\"Latitude\")\n",
    "plt.ylabel(\"Longitude\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
